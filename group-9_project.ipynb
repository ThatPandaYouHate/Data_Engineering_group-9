{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6ec6abd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b61b2cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_master_private_ip = '192.168.2.122'\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .master(f'spark://{spark_master_private_ip}:7077') \\\n",
    "        .appName('group-9_project')\\\n",
    "        .config('spark.dynamicAllocation.enabled', True)\\\n",
    "        .config('spark.dynamicAllocation.shuffleTracking.enabled',True)\\\n",
    "        .config('spark.shuffle.service.enabled', True)\\\n",
    "        .config('spark.dynamicAllocation.executorIdleTimeout','30s')\\\n",
    "        .config('spark.cores.max', 4)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "sql_context = SQLContext(spark.sparkContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "59aa5b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Loads the Reddit dataset containing comments as JSON documents from HDFS into a DataFrame \n",
    "# and stores it in cache memory to increase speed.\n",
    "reddit_comments = sql_context.read.json(f'hdfs://{spark_master_private_ip}:9000/group-9').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "037e5560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- author_flair_css_class: string (nullable = true)\n",
      " |-- author_flair_text: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- controversiality: long (nullable = true)\n",
      " |-- created_utc: long (nullable = true)\n",
      " |-- distinguished: string (nullable = true)\n",
      " |-- edited: boolean (nullable = true)\n",
      " |-- gilded: long (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- link_id: string (nullable = true)\n",
      " |-- parent_id: string (nullable = true)\n",
      " |-- retrieved_on: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- stickied: boolean (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- subreddit_id: string (nullable = true)\n",
      " |-- ups: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prints the schema of the Reddit comments. \n",
    "reddit_comments.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e05031b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the documents (comment bodies) of the Reddit comments. \n",
    "documents = reddit_comments.select(col('body').alias('document'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3a236dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            document|\n",
      "+--------------------+\n",
      "|A look at Vietnam...|\n",
      "|The site states \"...|\n",
      "|Jython related to...|\n",
      "|           [deleted]|\n",
      "|Saft is by far th...|\n",
      "|           [deleted]|\n",
      "|How to take panor...|\n",
      "|I donât know wh...|\n",
      "|LinkIt by Marc, a...|\n",
      "|Making websites r...|\n",
      "|On the bright sid...|\n",
      "|Like a lot of peo...|\n",
      "|This is comment t...|\n",
      "|           [deleted]|\n",
      "|           [deleted]|\n",
      "|           [deleted]|\n",
      "|           [deleted]|\n",
      "|It's a New York T...|\n",
      "|[Here's the copy ...|\n",
      "|The best thing ab...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prints the documents in the document corpus.\n",
    "documents.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "605d17c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processes each document in the document corpus. \n",
    "def pre_process(documents):\n",
    "    \n",
    "    # 1. Filters out deleted/removed documents\n",
    "    # 2. Removes URLs in the documents.\n",
    "    # 3. Converts the documents into lower case.\n",
    "    # 4. Removes non-alphabetic characters from the documents.\n",
    "    # 5. Trims the documents (removes leading and trailing spaces) \n",
    "    # 6. Removes empty documents \n",
    "    # 7. Tokenizes the documents (splits on whitespace)\n",
    "    \n",
    "    return documents.filter((col('document') != '[deleted]') & (col('document') != '[removed]')) \\\n",
    "                    .withColumn('document', regexp_replace(col('document'), 'http:\\S+|www\\.\\S+', '')) \\\n",
    "                    .withColumn('document', lower(col('document'))) \\\n",
    "                    .withColumn('document', regexp_replace(col('document'), \"[^a-z'\\s]\", ' ')) \\\n",
    "                    .withColumn('document', regexp_replace(col('document'), \"^\\s*|\\s*$\", '')) \\\n",
    "                    .filter(length(col('document')) > 0) \\\n",
    "                    .withColumn('document', split(col('document'), '\\s+')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ac54632b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            document|\n",
      "+--------------------+\n",
      "|[a, look, at, vie...|\n",
      "|[the, site, state...|\n",
      "|[jython, related,...|\n",
      "|[saft, is, by, fa...|\n",
      "|[how, to, take, p...|\n",
      "|[i, don, t, know,...|\n",
      "|[linkit, by, marc...|\n",
      "|[making, websites...|\n",
      "|[on, the, bright,...|\n",
      "|[like, a, lot, of...|\n",
      "|[this, is, commen...|\n",
      "|[it's, a, new, yo...|\n",
      "|[here's, the, cop...|\n",
      "|[the, best, thing...|\n",
      "|[you, can, rank, ...|\n",
      "|[just, testing, t...|\n",
      "|            [ye, ye]|\n",
      "|[we, didn't, tort...|\n",
      "|[interesting, art...|\n",
      "|[reddit, got, a, ...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pre-processes each document in the document corpus.\n",
    "documents = pre_process(documents)\n",
    "documents.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a75224bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|            document|doc_id|\n",
      "+--------------------+------+\n",
      "|[a, look, at, vie...|     0|\n",
      "|[the, site, state...|     1|\n",
      "|[jython, related,...|     2|\n",
      "|[saft, is, by, fa...|     3|\n",
      "|[how, to, take, p...|     4|\n",
      "|[i, don, t, know,...|     5|\n",
      "|[linkit, by, marc...|     6|\n",
      "|[making, websites...|     7|\n",
      "|[on, the, bright,...|     8|\n",
      "|[like, a, lot, of...|     9|\n",
      "|[this, is, commen...|    10|\n",
      "|[it's, a, new, yo...|    11|\n",
      "|[here's, the, cop...|    12|\n",
      "|[the, best, thing...|    13|\n",
      "|[you, can, rank, ...|    14|\n",
      "|[just, testing, t...|    15|\n",
      "|            [ye, ye]|    16|\n",
      "|[we, didn't, tort...|    17|\n",
      "|[interesting, art...|    18|\n",
      "|[reddit, got, a, ...|    19|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adds unique identifiers for all documents in the document corpus.  \n",
    "documents = documents.withColumn('doc_id', monotonically_increasing_id())\n",
    "documents.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6436cef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------+\n",
      "|            document|doc_id|         token|\n",
      "+--------------------+------+--------------+\n",
      "|[a, look, at, vie...|     0|             a|\n",
      "|[a, look, at, vie...|     0|          look|\n",
      "|[a, look, at, vie...|     0|            at|\n",
      "|[a, look, at, vie...|     0|       vietnam|\n",
      "|[a, look, at, vie...|     0|           and|\n",
      "|[a, look, at, vie...|     0|        mexico|\n",
      "|[a, look, at, vie...|     0|       exposes|\n",
      "|[a, look, at, vie...|     0|           the|\n",
      "|[a, look, at, vie...|     0|          myth|\n",
      "|[a, look, at, vie...|     0|            of|\n",
      "|[a, look, at, vie...|     0|        market|\n",
      "|[a, look, at, vie...|     0|liberalisation|\n",
      "|[the, site, state...|     1|           the|\n",
      "|[the, site, state...|     1|          site|\n",
      "|[the, site, state...|     1|        states|\n",
      "|[the, site, state...|     1|          what|\n",
      "|[the, site, state...|     1|           can|\n",
      "|[the, site, state...|     1|             i|\n",
      "|[the, site, state...|     1|           use|\n",
      "|[the, site, state...|     1|            it|\n",
      "+--------------------+------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Unfolds each document giving a set of tokens belonging to each document.\n",
    "unfolded_documents = documents.select(col('document'), col('doc_id'), explode(col('document')).alias('token')) \n",
    "unfolded_documents.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a2b927f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+--------------------+\n",
      "|doc_id|         token|                  tf|\n",
      "+------+--------------+--------------------+\n",
      "|     0|       exposes| 0.08333333333333333|\n",
      "|     0|           and| 0.08333333333333333|\n",
      "|     0|          myth| 0.08333333333333333|\n",
      "|     0|        mexico| 0.08333333333333333|\n",
      "|     0|            at| 0.08333333333333333|\n",
      "|     0|        market| 0.08333333333333333|\n",
      "|     0|       vietnam| 0.08333333333333333|\n",
      "|     0|           the| 0.08333333333333333|\n",
      "|     0|          look| 0.08333333333333333|\n",
      "|     0|             a| 0.08333333333333333|\n",
      "|     0|            of| 0.08333333333333333|\n",
      "|     0|liberalisation| 0.08333333333333333|\n",
      "|     1|         there|0.008849557522123894|\n",
      "|     1|          your|0.008849557522123894|\n",
      "|     1|         specs|0.017699115044247787|\n",
      "|     1|          they|0.008849557522123894|\n",
      "|     1|            me|0.017699115044247787|\n",
      "|     1|      personal|0.008849557522123894|\n",
      "|     1|       setting|0.008849557522123894|\n",
      "|     1|         these|0.008849557522123894|\n",
      "+------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Calculates the term frequencies (tf) for each document, \n",
    "# i.e. the number of occurrences of a term in a document divided by the total number of terms in the document.\n",
    "token_tf = unfolded_documents.groupBy('doc_id', 'token') \\\n",
    "                             .agg(count('document').alias('tf')) \\\n",
    "                             .withColumn('tf', col('tf') / sum('tf').over(Window.partitionBy('doc_id')))\n",
    "\n",
    "token_tf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "3d33a16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|token| df|\n",
      "+-----+---+\n",
      "|  the|626|\n",
      "|   to|529|\n",
      "|    a|478|\n",
      "|   of|456|\n",
      "|   is|433|\n",
      "|  and|428|\n",
      "|    i|420|\n",
      "| that|388|\n",
      "|   in|365|\n",
      "|   it|333|\n",
      "| this|324|\n",
      "|  for|295|\n",
      "|  you|268|\n",
      "|   on|261|\n",
      "|   be|243|\n",
      "|  not|229|\n",
      "| have|226|\n",
      "|  but|224|\n",
      "|  are|220|\n",
      "| with|207|\n",
      "+-----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculates the document frequencies (df) for each term, i.e. the number of documents having this term.  \n",
    "token_df = unfolded_documents.groupBy('token') \\\n",
    "                             .agg(countDistinct('doc_id').alias('df')) \\\n",
    "                             .sort('df', ascending=False) \n",
    "token_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0082e02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-------------------+\n",
      "|token| df|                idf|\n",
      "+-----+---+-------------------+\n",
      "|  the|626| 0.1811492720784181|\n",
      "|   to|529|0.25426793325366204|\n",
      "|    a|478|0.29829570867672883|\n",
      "|   of|456|0.31875876262441283|\n",
      "|   is|433|0.34123570893548233|\n",
      "|  and|428|0.34627983627567577|\n",
      "|    i|420| 0.3544743148909473|\n",
      "| that|388|0.38889187969464056|\n",
      "|   in|365| 0.4154307408323731|\n",
      "|   it|333|0.45527937178252786|\n",
      "| this|324|0.46717859508223564|\n",
      "|  for|295| 0.5079015893106847|\n",
      "|  you|268| 0.5495888112600589|\n",
      "|   on|261| 0.5610830979505668|\n",
      "|   be|243| 0.5921173316905356|\n",
      "|  not|229| 0.6178881229489598|\n",
      "| have|226| 0.6236151661414469|\n",
      "|  but|224|  0.627475586954685|\n",
      "|  are|220| 0.6353009244666415|\n",
      "| with|207|   0.66175325983193|\n",
      "+-----+---+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from math import log10\n",
    "\n",
    "# Total number of documents in the corpus.\n",
    "N = documents.count()\n",
    "\n",
    "# Calcutates the inverse document frequency (idf) of a term where\n",
    "# N is the total number of documents in the corpus and  \n",
    "# df is the document frequencies for that term.\n",
    "def idf(N, df):\n",
    "    return log10(N / df)\n",
    "\n",
    "# Defines an UDF for the idf() function\n",
    "udf_idf = udf(lambda df: idf(N, df))\n",
    "\n",
    "# Calculates the idf for each term \n",
    "token_idf = token_df.withColumn('idf', udf_idf(col('df')))\n",
    "\n",
    "token_idf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "5ed598bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+---+---+------------------+------------------+\n",
      "|          token|doc_id| tf| df|               idf|            tf_idf|\n",
      "+---------------+------+---+---+------------------+------------------+\n",
      "|         lalala|   398|1.0|  1|2.9777236052888476|2.9777236052888476|\n",
      "|         ewwwww|   159|1.0|  1|2.9777236052888476|2.9777236052888476|\n",
      "|            huh|   528|1.0|  1|2.9777236052888476|2.9777236052888476|\n",
      "|          yesss|   228|1.0|  1|2.9777236052888476|2.9777236052888476|\n",
      "|             ye|    16|1.0|  1|2.9777236052888476|2.9777236052888476|\n",
      "|noooooooooooooo|    37|1.0|  1|2.9777236052888476|2.9777236052888476|\n",
      "|           wimp|   372|1.0|  1|2.9777236052888476|2.9777236052888476|\n",
      "|      duplicate|   438|1.0|  2|2.6766936096248664|2.6766936096248664|\n",
      "|        moronic|   191|1.0|  2|2.6766936096248664|2.6766936096248664|\n",
      "|         agreed|   531|1.0|  3|2.5006023505691855|2.5006023505691855|\n",
      "|           dupe|   178|1.0|  3|2.5006023505691855|2.5006023505691855|\n",
      "|      hilarious|   177|1.0|  3|2.5006023505691855|2.5006023505691855|\n",
      "|      brilliant|   501|1.0|  5| 2.278753600952829| 2.278753600952829|\n",
      "|           nice|   349|1.0| 26|1.5627502573180299|1.5627502573180299|\n",
      "|            yes|   371|1.0| 28|1.5305655739466286|1.5305655739466286|\n",
      "|         whoops|   203|0.5|  1|2.9777236052888476|1.4888618026444238|\n",
      "|          wikis|   105|0.5|  1|2.9777236052888476|1.4888618026444238|\n",
      "|          awful|   244|0.5|  1|2.9777236052888476|1.4888618026444238|\n",
      "|         erotic|   556|0.5|  1|2.9777236052888476|1.4888618026444238|\n",
      "|         pixels|   556|0.5|  1|2.9777236052888476|1.4888618026444238|\n",
      "+---------------+------+---+---+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calcutates the term frequency–inverse document frequency (tf-idf) for each term as the product of the term's tf and idf.\n",
    "token_tf_idf = token_tf.join(token_idf, ['token']) \\\n",
    "                       .withColumn('tf_idf', col('tf') * col('idf')) \\\n",
    "                       .sort('tf_idf', ascending=False) \n",
    "token_tf_idf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f831e95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
