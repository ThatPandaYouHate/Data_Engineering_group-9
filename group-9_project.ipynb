{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6ec6abd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b61b2cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_master_private_ip = '192.168.2.122'\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .master(f'spark://{spark_master_private_ip}:7077') \\\n",
    "        .appName('group-9_project')\\\n",
    "        .config('spark.dynamicAllocation.enabled', True)\\\n",
    "        .config('spark.dynamicAllocation.shuffleTracking.enabled',True)\\\n",
    "        .config('spark.shuffle.service.enabled', True)\\\n",
    "        .config('spark.dynamicAllocation.executorIdleTimeout','30s')\\\n",
    "        .config('spark.cores.max', 4)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "sql_context = SQLContext(spark.sparkContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "59aa5b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Loads the Reddit dataset containing comments as JSON documents from HDFS into a DataFrame \n",
    "# and stores it in cache memory to increase speed.\n",
    "reddit_comments = sql_context.read.json(f'hdfs://{spark_master_private_ip}:9000/group-9').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "037e5560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- author_flair_css_class: string (nullable = true)\n",
      " |-- author_flair_text: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- controversiality: long (nullable = true)\n",
      " |-- created_utc: long (nullable = true)\n",
      " |-- distinguished: string (nullable = true)\n",
      " |-- edited: boolean (nullable = true)\n",
      " |-- gilded: long (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- link_id: string (nullable = true)\n",
      " |-- parent_id: string (nullable = true)\n",
      " |-- retrieved_on: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- stickied: boolean (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- subreddit_id: string (nullable = true)\n",
      " |-- ups: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prints the schema of the Reddit comments. \n",
    "reddit_comments.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e05031b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the documents (comment bodies) of the Reddit comments. \n",
    "documents = reddit_comments.select(col('body').alias('document'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3a236dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            document|\n",
      "+--------------------+\n",
      "|A look at Vietnam...|\n",
      "|The site states \"...|\n",
      "|Jython related to...|\n",
      "|           [deleted]|\n",
      "|Saft is by far th...|\n",
      "|           [deleted]|\n",
      "|How to take panor...|\n",
      "|I donât know wh...|\n",
      "|LinkIt by Marc, a...|\n",
      "|Making websites r...|\n",
      "|On the bright sid...|\n",
      "|Like a lot of peo...|\n",
      "|This is comment t...|\n",
      "|           [deleted]|\n",
      "|           [deleted]|\n",
      "|           [deleted]|\n",
      "|           [deleted]|\n",
      "|It's a New York T...|\n",
      "|[Here's the copy ...|\n",
      "|The best thing ab...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prints the documents in the document corpus.\n",
    "documents.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "605d17c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processes each document in the document corpus. \n",
    "def pre_process(documents):\n",
    "    # Lowercases the documents.\n",
    "    lower_words = lower(col('document'))\n",
    "    \n",
    "    # Tokenizes the documents (splits on whitespace). \n",
    "    whitespace = '\\\\s+'\n",
    "    tokenize_words = split(lower_words, whitespace)\n",
    "    \n",
    "    # Filters out deleted/removed documents and tokenization\n",
    "    return documents.filter((col('document') != '[deleted]') & (col('document') != '[removed]')) \\\n",
    "                    .select(tokenize_words.alias('document'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ac54632b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            document|\n",
      "+--------------------+\n",
      "|[a, look, at, vie...|\n",
      "|[the, site, state...|\n",
      "|[jython, related,...|\n",
      "|[saft, is, by, fa...|\n",
      "|[how, to, take, p...|\n",
      "|[i, donât, know...|\n",
      "|[linkit, by, marc...|\n",
      "|[making, websites...|\n",
      "|[on, the, bright,...|\n",
      "|[like, a, lot, of...|\n",
      "|[this, is, commen...|\n",
      "|[it's, a, new, yo...|\n",
      "|[[here's, the, co...|\n",
      "|[the, best, thing...|\n",
      "|[you, can, rank, ...|\n",
      "|[just, testing, t...|\n",
      "|            [ye, ye]|\n",
      "|[_we, didn't, tor...|\n",
      "|[interesting, art...|\n",
      "|[reddit, got, a, ...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pre-processes each document in the document corpus.\n",
    "documents = pre_process(documents)\n",
    "documents.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a75224bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|            document|doc_id|\n",
      "+--------------------+------+\n",
      "|[a, look, at, vie...|     0|\n",
      "|[the, site, state...|     1|\n",
      "|[jython, related,...|     2|\n",
      "|[saft, is, by, fa...|     3|\n",
      "|[how, to, take, p...|     4|\n",
      "|[i, donât, know...|     5|\n",
      "|[linkit, by, marc...|     6|\n",
      "|[making, websites...|     7|\n",
      "|[on, the, bright,...|     8|\n",
      "|[like, a, lot, of...|     9|\n",
      "|[this, is, commen...|    10|\n",
      "|[it's, a, new, yo...|    11|\n",
      "|[[here's, the, co...|    12|\n",
      "|[the, best, thing...|    13|\n",
      "|[you, can, rank, ...|    14|\n",
      "|[just, testing, t...|    15|\n",
      "|            [ye, ye]|    16|\n",
      "|[_we, didn't, tor...|    17|\n",
      "|[interesting, art...|    18|\n",
      "|[reddit, got, a, ...|    19|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adds unique identifiers for all documents in the document corpus.  \n",
    "documents = documents.withColumn('doc_id', monotonically_increasing_id())\n",
    "documents.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6436cef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+---------------+\n",
      "|            document|doc_id|          token|\n",
      "+--------------------+------+---------------+\n",
      "|[a, look, at, vie...|     0|              a|\n",
      "|[a, look, at, vie...|     0|           look|\n",
      "|[a, look, at, vie...|     0|             at|\n",
      "|[a, look, at, vie...|     0|        vietnam|\n",
      "|[a, look, at, vie...|     0|            and|\n",
      "|[a, look, at, vie...|     0|         mexico|\n",
      "|[a, look, at, vie...|     0|        exposes|\n",
      "|[a, look, at, vie...|     0|            the|\n",
      "|[a, look, at, vie...|     0|           myth|\n",
      "|[a, look, at, vie...|     0|             of|\n",
      "|[a, look, at, vie...|     0|         market|\n",
      "|[a, look, at, vie...|     0|liberalisation.|\n",
      "|[the, site, state...|     1|            the|\n",
      "|[the, site, state...|     1|           site|\n",
      "|[the, site, state...|     1|         states|\n",
      "|[the, site, state...|     1|          \"what|\n",
      "|[the, site, state...|     1|            can|\n",
      "|[the, site, state...|     1|              i|\n",
      "|[the, site, state...|     1|            use|\n",
      "|[the, site, state...|     1|             it|\n",
      "+--------------------+------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Unfolds each document giving a set of tokens belonging to each document.\n",
    "unfolded_documents = documents.select(col('document'), col('doc_id'), explode(col('document')).alias('token'))\n",
    "unfolded_documents.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a2b927f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+--------------------+\n",
      "|doc_id|          token|                  tf|\n",
      "+------+---------------+--------------------+\n",
      "|     0|        exposes| 0.08333333333333333|\n",
      "|     0|            and| 0.08333333333333333|\n",
      "|     0|           myth| 0.08333333333333333|\n",
      "|     0|         mexico| 0.08333333333333333|\n",
      "|     0|             at| 0.08333333333333333|\n",
      "|     0|         market| 0.08333333333333333|\n",
      "|     0|        vietnam| 0.08333333333333333|\n",
      "|     0|            the| 0.08333333333333333|\n",
      "|     0|           look| 0.08333333333333333|\n",
      "|     0|liberalisation.| 0.08333333333333333|\n",
      "|     0|              a| 0.08333333333333333|\n",
      "|     0|             of| 0.08333333333333333|\n",
      "|     1|          there|0.009009009009009009|\n",
      "|     1|           your|0.009009009009009009|\n",
      "|     1|          specs|0.018018018018018018|\n",
      "|     1|           they|0.009009009009009009|\n",
      "|     1|           web.|0.009009009009009009|\n",
      "|     1|             me|0.018018018018018018|\n",
      "|     1|       personal|0.009009009009009009|\n",
      "|     1|        setting|0.009009009009009009|\n",
      "+------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Calculates the term frequencies (tf) for each document, \n",
    "# i.e. the number of occurrences of a term in a document divided by the total number of terms in the document.\n",
    "token_tf = unfolded_documents.groupBy('doc_id', 'token') \\\n",
    "                             .agg(count('document').alias('tf')) \\\n",
    "                             .withColumn('tf', col('tf') / sum('tf').over(Window.partitionBy('doc_id')))\n",
    "\n",
    "token_tf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3d33a16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|token| df|\n",
      "+-----+---+\n",
      "|  the|618|\n",
      "|   to|523|\n",
      "|    a|476|\n",
      "|   of|452|\n",
      "|   is|427|\n",
      "|  and|421|\n",
      "|    i|412|\n",
      "| that|374|\n",
      "|   in|359|\n",
      "|   it|302|\n",
      "| this|300|\n",
      "|  for|291|\n",
      "|  you|258|\n",
      "|   on|247|\n",
      "|   be|242|\n",
      "| have|224|\n",
      "|  but|218|\n",
      "|  are|214|\n",
      "|  not|212|\n",
      "| with|204|\n",
      "+-----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculates the document frequencies (df) for each term, i.e. the number of documents having this term.  \n",
    "token_df = unfolded_documents.groupBy('token') \\\n",
    "                             .agg(countDistinct('doc_id').alias('df')) \\\n",
    "                             .sort('df', ascending=False) \n",
    "token_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0082e02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-------------------+\n",
      "|token| df|                idf|\n",
      "+-----+---+-------------------+\n",
      "|  the|618|0.18901489649493047|\n",
      "|   to|523| 0.2615016827164721|\n",
      "|    a|476| 0.3023964188632532|\n",
      "|   of|452| 0.3248649367723642|\n",
      "|   is|427| 0.3495754965587225|\n",
      "|  and|421|  0.355721275748078|\n",
      "|    i|412|0.36510615555061177|\n",
      "| that|374|0.40713176938326623|\n",
      "|   in|359|0.42490892300542715|\n",
      "|   it|302|0.49999642862659566|\n",
      "| this|300| 0.5028821168640839|\n",
      "|  for|291|  0.516110382597839|\n",
      "|  you|258| 0.5683836656205161|\n",
      "|   on|247| 0.5873064183240806|\n",
      "|   be|242| 0.5961880056033151|\n",
      "| have|224| 0.6297553532495835|\n",
      "|  but|218| 0.6415468779791416|\n",
      "|  are|214| 0.6495895982345555|\n",
      "|  not|212| 0.6536675106549948|\n",
      "| with|204| 0.6703732041578476|\n",
      "+-----+---+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from math import log10\n",
    "\n",
    "# Total number of documents in the corpus.\n",
    "N = documents.count()\n",
    "\n",
    "# Calcutates the inverse document frequency (idf) of a term where\n",
    "# N is the total number of documents in the corpus and  \n",
    "# df is the document frequencies for that term.\n",
    "def idf(N, df):\n",
    "    return log10(N / df)\n",
    "\n",
    "# Defines an UDF for the idf() function\n",
    "udf_idf = udf(lambda df: idf(N, df))\n",
    "\n",
    "# Calculates the idf for each term \n",
    "token_idf = token_df.withColumn('idf', udf_idf(col('df')))\n",
    "\n",
    "token_idf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5ed598bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+--------------------+---+-------------------+--------------------+\n",
      "|          token|doc_id|                  tf| df|                idf|              tf_idf|\n",
      "+---------------+------+--------------------+---+-------------------+--------------------+\n",
      "|        exposes|     0| 0.08333333333333333|  3|  2.502882116864084| 0.20857350973867364|\n",
      "|            and|     0| 0.08333333333333333|421|  0.355721275748078|0.029643439645673165|\n",
      "|           myth|     0| 0.08333333333333333|  4|  2.377943380255784| 0.19816194835464865|\n",
      "|         mexico|     0| 0.08333333333333333|  1| 2.9800033715837464| 0.24833361429864553|\n",
      "|             at|     0| 0.08333333333333333|160| 0.7758833889278216| 0.06465694907731846|\n",
      "|         market|     0| 0.08333333333333333|  6| 2.2018521212001025|  0.1834876767666752|\n",
      "|        vietnam|     0| 0.08333333333333333|  2|  2.678973375919765| 0.22324778132664708|\n",
      "|            the|     0| 0.08333333333333333|618|0.18901489649493047| 0.01575124137457754|\n",
      "|           look|     0| 0.08333333333333333| 23| 1.6182755355661533| 0.13485629463051277|\n",
      "|liberalisation.|     0| 0.08333333333333333|  1| 2.9800033715837464| 0.24833361429864553|\n",
      "|              a|     0| 0.08333333333333333|476| 0.3023964188632532|0.025199701571937766|\n",
      "|             of|     0| 0.08333333333333333|452| 0.3248649367723642|0.027072078064363684|\n",
      "|          there|     1|0.009009009009009009| 96| 0.9977321385441779|0.008988577824722323|\n",
      "|           your|     1|0.009009009009009009|109| 0.9425768736431227| 0.00849168354633444|\n",
      "|          specs|     1|0.018018018018018018|  1| 2.9800033715837464| 0.05369375444295039|\n",
      "|           they|     1|0.009009009009009009|153| 0.7953119407661475|0.007164972439334663|\n",
      "|           web.|     1|0.009009009009009009|  3|  2.502882116864084| 0.02254848753931607|\n",
      "|             me|     1|0.018018018018018018| 82| 1.0661895192000297|0.019210621967568103|\n",
      "|       personal|     1|0.009009009009009009|  6| 2.2018521212001025| 0.01983650559639732|\n",
      "|        setting|     1|0.009009009009009009|  1| 2.9800033715837464|0.026846877221475194|\n",
      "+---------------+------+--------------------+---+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calcutates the term frequency–inverse document frequency (tf-idf) for each term as the product of the term's tf and idf.\n",
    "token_tf_idf = token_tf.join(token_idf, ['token']) \\\n",
    "                       .withColumn('tf_idf', col('tf') * col('idf')) \n",
    "token_tf_idf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "22155d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
